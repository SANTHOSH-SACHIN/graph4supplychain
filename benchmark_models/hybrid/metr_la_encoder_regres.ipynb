{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install"
      ],
      "metadata": {
        "id": "4Zr2IdBUnVJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9icIAb_JDq7",
        "outputId": "56f083a8-10bd-4615-a63f-392f59e37ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (494 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3%2Bpt20cpu-cp310-cp310-linux_x86_64.whl (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.3/751.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt20cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt20cpu torch-sparse-0.6.18+pt20cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMDYkpnqJMQM",
        "outputId": "4bdd250a-1eb5-4368-f496-74431d0d633d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# encoder_decoder_gnn_Regression Task"
      ],
      "metadata": {
        "id": "f9L7RIYve0GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/*"
      ],
      "metadata": {
        "id": "cNAe5Jwvk6BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to a different directory\n",
        "drive.mount('/content/drive_mount')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfHyAXPskrxm",
        "outputId": "0ea44a11-5e01-4ce4-d339-d5e18fd9ba88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive_mount\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SI-mN1jUplqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eI7dITMplso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load METR-LA dataset\n",
        "def load_metr_la_dataset(adj_path, time_series_path):\n",
        "    adj_data = pd.read_pickle(adj_path)\n",
        "    if isinstance(adj_data, list):\n",
        "        adj_matrix = np.array([list(map(float, row)) for row in adj_data[0]], dtype=np.float32)\n",
        "        node_mapping = adj_data[1]\n",
        "    elif isinstance(adj_data, dict):\n",
        "        adj_matrix = adj_data[\"adjacency_matrix\"]\n",
        "        node_mapping = adj_data[\"node_mapping\"]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected structure in adj_METR-LA.pkl\")\n",
        "\n",
        "    if len(adj_matrix.shape) == 1:\n",
        "        num_nodes = len(adj_matrix)\n",
        "        adj_matrix = np.diag(adj_matrix)\n",
        "    elif len(adj_matrix.shape) != 2:\n",
        "        raise ValueError(f\"Cannot convert adjacency matrix to 2D. Current shape: {adj_matrix.shape}\")\n",
        "\n",
        "    time_series_data = pd.read_hdf(time_series_path, \"df\")  # shape: (timesteps, nodes)\n",
        "    return adj_matrix, node_mapping, time_series_data\n",
        "\n",
        "# Generate static node features from time-series\n",
        "def create_node_features(time_series_data, aggregation=\"mean\"):\n",
        "    if aggregation == \"mean\":\n",
        "        node_features = time_series_data.mean(axis=0).values\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported aggregation method.\")\n",
        "    node_features = np.array(node_features, dtype=np.float32)\n",
        "    node_features = np.expand_dims(node_features, axis=1)\n",
        "    return torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "# Convert adjacency matrix to edge list for PyTorch Geometric\n",
        "def adjacency_to_edge_list(adj_matrix):\n",
        "    if isinstance(adj_matrix, pd.DataFrame):\n",
        "        adj_matrix = adj_matrix.values\n",
        "\n",
        "    edge_indices = np.nonzero(adj_matrix)\n",
        "    edge_index = np.stack(edge_indices, axis=0)\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    return edge_index\n",
        "\n",
        "# Prepare PyTorch Geometric Data object\n",
        "def prepare_graph_data(adj_matrix, node_features, node_labels, train_mask, test_mask):\n",
        "    edge_index = adjacency_to_edge_list(adj_matrix)\n",
        "    data = Data(\n",
        "        x=node_features,\n",
        "        edge_index=edge_index,\n",
        "        y=torch.tensor(node_labels, dtype=torch.float),\n",
        "        train_mask=train_mask,\n",
        "        test_mask=test_mask\n",
        "    )\n",
        "    return data\n",
        "\n",
        "# Define GNNEncoder\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Define NodeRegressor\n",
        "class NodeRegressor(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define overall Model\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, encoder, regressor):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.regressor = regressor\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = self.encoder(data.x, data.edge_index)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, data, optimizer, criterion, epochs=100, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    data = data.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask].unsqueeze(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation function with accuracy metrics added\n",
        "def evaluate_model(model, data, device=\"cpu\", accuracy_threshold=0.05):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "    pred = out.squeeze().cpu().numpy()\n",
        "    true = data.y.cpu().numpy()\n",
        "\n",
        "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
        "    def mape(y_true, y_pred):\n",
        "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "    # Calculate accuracy (based on a threshold for \"correct\" predictions)\n",
        "    def accuracy(y_true, y_pred, threshold=accuracy_threshold):\n",
        "        return np.mean(np.abs((y_true - y_pred) / y_true) < threshold) * 100\n",
        "\n",
        "    mse_train = mean_squared_error(true[data.train_mask], pred[data.train_mask])\n",
        "    mae_train = mean_absolute_error(true[data.train_mask], pred[data.train_mask])\n",
        "    mape_train = mape(true[data.train_mask], pred[data.train_mask])\n",
        "    r2_train = r2_score(true[data.train_mask], pred[data.train_mask])\n",
        "    acc_train = accuracy(true[data.train_mask], pred[data.train_mask])\n",
        "\n",
        "    mse_test = mean_squared_error(true[data.test_mask], pred[data.test_mask])\n",
        "    mae_test = mean_absolute_error(true[data.test_mask], pred[data.test_mask])\n",
        "    mape_test = mape(true[data.test_mask], pred[data.test_mask])\n",
        "    r2_test = r2_score(true[data.test_mask], pred[data.test_mask])\n",
        "    acc_test = accuracy(true[data.test_mask], pred[data.test_mask])\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Train MSE: {mse_train:.4f}, MAE: {mae_train:.4f}, MAPE: {mape_train:.4f}, R²: {r2_train:.4f}, Accuracy: {acc_train:.4f}%\")\n",
        "    print(f\"Test MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, MAPE: {mape_test:.4f}, R²: {r2_test:.4f}, Accuracy: {acc_test:.4f}%\")\n",
        "\n",
        "# Paths to METR-LA dataset\n",
        "adj_path = \"/content/drive_mount/MyDrive/archive (29)/adj_METR-LA.pkl\"\n",
        "time_series_path = \"/content/drive_mount/MyDrive/archive (29)/METR-LA.h5\"\n",
        "\n",
        "# Load dataset\n",
        "adj_matrix, node_mapping, time_series_data = load_metr_la_dataset(adj_path, time_series_path)\n",
        "\n",
        "# Create node features (mean of time series)\n",
        "node_features = create_node_features(time_series_data, aggregation=\"mean\")\n",
        "\n",
        "# Create node labels (standard deviation of time series)\n",
        "node_labels = time_series_data.std(axis=0).values\n",
        "\n",
        "# Define train and test masks\n",
        "num_nodes = node_features.shape[0]\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * num_nodes)\n",
        "perm = torch.randperm(num_nodes)\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[perm[:train_size]] = True\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask[perm[train_size:]] = True\n",
        "\n",
        "# Prepare graph data\n",
        "graph_data = prepare_graph_data(adj_matrix, node_features, node_labels, train_mask, test_mask)\n",
        "\n",
        "# Define model components\n",
        "input_dim = node_features.size(1)\n",
        "hidden_dim = 64\n",
        "out_dim = 32\n",
        "\n",
        "encoder = GNNEncoder(input_dim, hidden_dim, out_dim)\n",
        "regressor = NodeRegressor(out_dim)\n",
        "model = Model(encoder, regressor)\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, graph_data, optimizer, criterion, epochs=100, device=device)\n",
        "evaluate_model(model, graph_data, device=device)\n"
      ],
      "metadata": {
        "id": "WzrxFEVQub5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20bb0a6-2cc6-4332-92df-0075aae9a35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 373.2015\n",
            "Epoch 2/100, Loss: 125.8073\n",
            "Epoch 3/100, Loss: 236.5612\n",
            "Epoch 4/100, Loss: 65.2744\n",
            "Epoch 5/100, Loss: 16.7371\n",
            "Epoch 6/100, Loss: 83.4525\n",
            "Epoch 7/100, Loss: 111.6744\n",
            "Epoch 8/100, Loss: 76.3283\n",
            "Epoch 9/100, Loss: 30.1711\n",
            "Epoch 10/100, Loss: 12.3061\n",
            "Epoch 11/100, Loss: 24.9665\n",
            "Epoch 12/100, Loss: 44.8249\n",
            "Epoch 13/100, Loss: 49.6002\n",
            "Epoch 14/100, Loss: 37.5844\n",
            "Epoch 15/100, Loss: 21.6281\n",
            "Epoch 16/100, Loss: 13.0486\n",
            "Epoch 17/100, Loss: 14.2195\n",
            "Epoch 18/100, Loss: 20.7556\n",
            "Epoch 19/100, Loss: 26.9118\n",
            "Epoch 20/100, Loss: 29.1794\n",
            "Epoch 21/100, Loss: 26.9555\n",
            "Epoch 22/100, Loss: 21.8270\n",
            "Epoch 23/100, Loss: 16.3475\n",
            "Epoch 24/100, Loss: 12.8693\n",
            "Epoch 25/100, Loss: 12.5793\n",
            "Epoch 26/100, Loss: 14.8445\n",
            "Epoch 27/100, Loss: 17.5955\n",
            "Epoch 28/100, Loss: 18.8710\n",
            "Epoch 29/100, Loss: 17.9922\n",
            "Epoch 30/100, Loss: 15.6990\n",
            "Epoch 31/100, Loss: 13.4456\n",
            "Epoch 32/100, Loss: 12.3494\n",
            "Epoch 33/100, Loss: 12.6246\n",
            "Epoch 34/100, Loss: 13.7163\n",
            "Epoch 35/100, Loss: 14.7945\n",
            "Epoch 36/100, Loss: 15.2299\n",
            "Epoch 37/100, Loss: 14.8435\n",
            "Epoch 38/100, Loss: 13.9009\n",
            "Epoch 39/100, Loss: 12.9125\n",
            "Epoch 40/100, Loss: 12.3498\n",
            "Epoch 41/100, Loss: 12.4016\n",
            "Epoch 42/100, Loss: 12.8923\n",
            "Epoch 43/100, Loss: 13.4151\n",
            "Epoch 44/100, Loss: 13.6039\n",
            "Epoch 45/100, Loss: 13.3586\n",
            "Epoch 46/100, Loss: 12.8682\n",
            "Epoch 47/100, Loss: 12.4408\n",
            "Epoch 48/100, Loss: 12.2887\n",
            "Epoch 49/100, Loss: 12.4204\n",
            "Epoch 50/100, Loss: 12.6787\n",
            "Epoch 51/100, Loss: 12.8666\n",
            "Epoch 52/100, Loss: 12.8668\n",
            "Epoch 53/100, Loss: 12.6933\n",
            "Epoch 54/100, Loss: 12.4609\n",
            "Epoch 55/100, Loss: 12.3038\n",
            "Epoch 56/100, Loss: 12.2992\n",
            "Epoch 57/100, Loss: 12.4074\n",
            "Epoch 58/100, Loss: 12.5077\n",
            "Epoch 59/100, Loss: 12.5342\n",
            "Epoch 60/100, Loss: 12.4591\n",
            "Epoch 61/100, Loss: 12.3425\n",
            "Epoch 62/100, Loss: 12.2746\n",
            "Epoch 63/100, Loss: 12.3075\n",
            "Epoch 64/100, Loss: 12.3767\n",
            "Epoch 65/100, Loss: 12.3869\n",
            "Epoch 66/100, Loss: 12.3408\n",
            "Epoch 67/100, Loss: 12.2863\n",
            "Epoch 68/100, Loss: 12.2617\n",
            "Epoch 69/100, Loss: 12.2734\n",
            "Epoch 70/100, Loss: 12.2941\n",
            "Epoch 71/100, Loss: 12.3051\n",
            "Epoch 72/100, Loss: 12.2945\n",
            "Epoch 73/100, Loss: 12.2727\n",
            "Epoch 74/100, Loss: 12.2553\n",
            "Epoch 75/100, Loss: 12.2518\n",
            "Epoch 76/100, Loss: 12.2597\n",
            "Epoch 77/100, Loss: 12.2691\n",
            "Epoch 78/100, Loss: 12.2697\n",
            "Epoch 79/100, Loss: 12.2603\n",
            "Epoch 80/100, Loss: 12.2463\n",
            "Epoch 81/100, Loss: 12.2354\n",
            "Epoch 82/100, Loss: 12.2316\n",
            "Epoch 83/100, Loss: 12.2328\n",
            "Epoch 84/100, Loss: 12.2330\n",
            "Epoch 85/100, Loss: 12.2293\n",
            "Epoch 86/100, Loss: 12.2229\n",
            "Epoch 87/100, Loss: 12.2151\n",
            "Epoch 88/100, Loss: 12.2101\n",
            "Epoch 89/100, Loss: 12.2089\n",
            "Epoch 90/100, Loss: 12.2096\n",
            "Epoch 91/100, Loss: 12.2088\n",
            "Epoch 92/100, Loss: 12.2052\n",
            "Epoch 93/100, Loss: 12.1997\n",
            "Epoch 94/100, Loss: 12.1948\n",
            "Epoch 95/100, Loss: 12.1919\n",
            "Epoch 96/100, Loss: 12.1907\n",
            "Epoch 97/100, Loss: 12.1895\n",
            "Epoch 98/100, Loss: 12.1869\n",
            "Epoch 99/100, Loss: 12.1828\n",
            "Epoch 100/100, Loss: 12.1783\n",
            "Train MSE: 12.1748, MAE: 2.7114, MAPE: 13.7945, R²: -0.6572, Accuracy: 21.8182%\n",
            "Test MSE: 9.3953, MAE: 2.4396, MAPE: 12.9538, R²: -1.1106, Accuracy: 28.5714%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tb3vOsb-I-2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CgrgdNnXxWA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder_decoder_ClassificationTask"
      ],
      "metadata": {
        "id": "Dr9dEHFcxWIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load METR-LA dataset\n",
        "def load_metr_la_dataset(adj_path, time_series_path):\n",
        "    adj_data = pd.read_pickle(adj_path)\n",
        "    if isinstance(adj_data, list):\n",
        "        adj_matrix = np.array([list(map(float, row)) for row in adj_data[0]], dtype=np.float32)\n",
        "        node_mapping = adj_data[1]\n",
        "    elif isinstance(adj_data, dict):\n",
        "        adj_matrix = adj_data[\"adjacency_matrix\"]\n",
        "        node_mapping = adj_data[\"node_mapping\"]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected structure in adj_METR-LA.pkl\")\n",
        "\n",
        "    if len(adj_matrix.shape) == 1:\n",
        "        num_nodes = len(adj_matrix)\n",
        "        adj_matrix = np.diag(adj_matrix)\n",
        "    elif len(adj_matrix.shape) != 2:\n",
        "        raise ValueError(f\"Cannot convert adjacency matrix to 2D. Current shape: {adj_matrix.shape}\")\n",
        "\n",
        "    time_series_data = pd.read_hdf(time_series_path, \"df\")  # shape: (timesteps, nodes)\n",
        "    return adj_matrix, node_mapping, time_series_data\n",
        "\n",
        "# Generate static node features from time-series\n",
        "def create_node_features(time_series_data, aggregation=\"mean\"):\n",
        "    if aggregation == \"mean\":\n",
        "        node_features = time_series_data.mean(axis=0).values\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported aggregation method.\")\n",
        "    node_features = np.array(node_features, dtype=np.float32)\n",
        "    node_features = np.expand_dims(node_features, axis=1)\n",
        "    return torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "# Convert adjacency matrix to edge list for PyTorch Geometric\n",
        "def adjacency_to_edge_list(adj_matrix):\n",
        "    if isinstance(adj_matrix, pd.DataFrame):\n",
        "        adj_matrix = adj_matrix.values\n",
        "\n",
        "    edge_indices = np.nonzero(adj_matrix)\n",
        "    edge_index = np.stack(edge_indices, axis=0)\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    return edge_index\n",
        "\n",
        "# Prepare PyTorch Geometric Data object\n",
        "def prepare_graph_data(adj_matrix, node_features, node_labels, train_mask, test_mask):\n",
        "    edge_index = adjacency_to_edge_list(adj_matrix)\n",
        "    data = Data(\n",
        "        x=node_features,\n",
        "        edge_index=edge_index,\n",
        "        y=torch.tensor(node_labels, dtype=torch.long),\n",
        "        train_mask=train_mask,\n",
        "        test_mask=test_mask\n",
        "    )\n",
        "    return data\n",
        "\n",
        "# Define GNNEncoder\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Define NodeClassifier\n",
        "class NodeClassifier(torch.nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define overall Model\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, encoder, classifier):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = classifier\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = self.encoder(data.x, data.edge_index)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, data, optimizer, criterion, epochs=100, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    data = data.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, data, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "    pred = out.argmax(dim=1).cpu()\n",
        "    true = data.y.cpu()\n",
        "\n",
        "    acc_train = accuracy_score(true[data.train_mask], pred[data.train_mask])\n",
        "    acc_test = accuracy_score(true[data.test_mask], pred[data.test_mask])\n",
        "\n",
        "    print(f\"Train Accuracy: {acc_train:.4f}\")\n",
        "    print(f\"Test Accuracy: {acc_test:.4f}\")\n",
        "\n",
        "# Paths to METR-LA dataset\n",
        "adj_path = \"/content/drive_mount/MyDrive/archive (29)/adj_METR-LA.pkl\"\n",
        "time_series_path = \"/content/drive_mount/MyDrive/archive (29)/METR-LA.h5\"\n",
        "\n",
        "# Load dataset\n",
        "adj_matrix, node_mapping, time_series_data = load_metr_la_dataset(adj_path, time_series_path)\n",
        "\n",
        "# Create node features (mean of time series)\n",
        "node_features = create_node_features(time_series_data, aggregation=\"mean\")\n",
        "\n",
        "# Create node labels (standard deviation of time series)\n",
        "node_labels = time_series_data.std(axis=0).values\n",
        "\n",
        "# Convert node labels to discrete classes\n",
        "num_classes = 3\n",
        "node_labels = pd.qcut(node_labels, q=num_classes, labels=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-nG49RxWx0dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define train and test masks\n",
        "num_nodes = node_features.shape[0]\n",
        "train_ratio = 0.8\n",
        "train_size = int(train_ratio * num_nodes)\n",
        "perm = torch.randperm(num_nodes)\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[perm[:train_size]] = True\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask[perm[train_size:]] = True\n",
        "\n",
        "# Prepare graph data\n",
        "graph_data = prepare_graph_data(adj_matrix, node_features, node_labels, train_mask, test_mask)\n",
        "\n",
        "# Define model components\n",
        "input_dim = node_features.size(1)\n",
        "hidden_dim = 64\n",
        "out_dim = 32\n",
        "num_classes = len(np.unique(node_labels))\n",
        "\n",
        "encoder = GNNEncoder(input_dim, hidden_dim, out_dim)\n",
        "classifier = NodeClassifier(out_dim, num_classes)\n",
        "model = Model(encoder, classifier)\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, graph_data, optimizer, criterion, epochs=100, device=device)\n",
        "evaluate_model(model, graph_data, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGpbLg_eyiry",
        "outputId": "dfabf662-2b06-4b7a-9ab9-6be4f3e6d99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 5.7422\n",
            "Epoch 2/100, Loss: 16.7089\n",
            "Epoch 3/100, Loss: 18.3224\n",
            "Epoch 4/100, Loss: 15.6903\n",
            "Epoch 5/100, Loss: 10.3265\n",
            "Epoch 6/100, Loss: 7.8031\n",
            "Epoch 7/100, Loss: 4.3610\n",
            "Epoch 8/100, Loss: 2.9355\n",
            "Epoch 9/100, Loss: 4.8440\n",
            "Epoch 10/100, Loss: 3.4965\n",
            "Epoch 11/100, Loss: 1.3875\n",
            "Epoch 12/100, Loss: 2.2390\n",
            "Epoch 13/100, Loss: 2.7147\n",
            "Epoch 14/100, Loss: 3.1564\n",
            "Epoch 15/100, Loss: 2.6710\n",
            "Epoch 16/100, Loss: 2.5091\n",
            "Epoch 17/100, Loss: 1.7229\n",
            "Epoch 18/100, Loss: 1.3881\n",
            "Epoch 19/100, Loss: 1.7822\n",
            "Epoch 20/100, Loss: 2.0783\n",
            "Epoch 21/100, Loss: 1.7735\n",
            "Epoch 22/100, Loss: 1.3553\n",
            "Epoch 23/100, Loss: 1.4350\n",
            "Epoch 24/100, Loss: 1.3148\n",
            "Epoch 25/100, Loss: 1.5452\n",
            "Epoch 26/100, Loss: 1.5196\n",
            "Epoch 27/100, Loss: 1.2639\n",
            "Epoch 28/100, Loss: 1.3432\n",
            "Epoch 29/100, Loss: 1.1697\n",
            "Epoch 30/100, Loss: 1.2390\n",
            "Epoch 31/100, Loss: 1.3610\n",
            "Epoch 32/100, Loss: 1.2848\n",
            "Epoch 33/100, Loss: 1.1567\n",
            "Epoch 34/100, Loss: 1.2090\n",
            "Epoch 35/100, Loss: 1.1816\n",
            "Epoch 36/100, Loss: 1.1196\n",
            "Epoch 37/100, Loss: 1.2004\n",
            "Epoch 38/100, Loss: 1.1974\n",
            "Epoch 39/100, Loss: 1.1580\n",
            "Epoch 40/100, Loss: 1.1774\n",
            "Epoch 41/100, Loss: 1.1317\n",
            "Epoch 42/100, Loss: 1.1372\n",
            "Epoch 43/100, Loss: 1.1648\n",
            "Epoch 44/100, Loss: 1.1358\n",
            "Epoch 45/100, Loss: 1.1152\n",
            "Epoch 46/100, Loss: 1.1356\n",
            "Epoch 47/100, Loss: 1.1129\n",
            "Epoch 48/100, Loss: 1.1016\n",
            "Epoch 49/100, Loss: 1.1140\n",
            "Epoch 50/100, Loss: 1.1012\n",
            "Epoch 51/100, Loss: 1.1103\n",
            "Epoch 52/100, Loss: 1.1222\n",
            "Epoch 53/100, Loss: 1.1009\n",
            "Epoch 54/100, Loss: 1.1035\n",
            "Epoch 55/100, Loss: 1.1001\n",
            "Epoch 56/100, Loss: 1.0929\n",
            "Epoch 57/100, Loss: 1.1002\n",
            "Epoch 58/100, Loss: 1.0944\n",
            "Epoch 59/100, Loss: 1.0965\n",
            "Epoch 60/100, Loss: 1.1027\n",
            "Epoch 61/100, Loss: 1.0939\n",
            "Epoch 62/100, Loss: 1.0942\n",
            "Epoch 63/100, Loss: 1.0949\n",
            "Epoch 64/100, Loss: 1.0895\n",
            "Epoch 65/100, Loss: 1.0919\n",
            "Epoch 66/100, Loss: 1.0901\n",
            "Epoch 67/100, Loss: 1.0912\n",
            "Epoch 68/100, Loss: 1.0923\n",
            "Epoch 69/100, Loss: 1.0890\n",
            "Epoch 70/100, Loss: 1.0907\n",
            "Epoch 71/100, Loss: 1.0898\n",
            "Epoch 72/100, Loss: 1.0877\n",
            "Epoch 73/100, Loss: 1.0890\n",
            "Epoch 74/100, Loss: 1.0885\n",
            "Epoch 75/100, Loss: 1.0888\n",
            "Epoch 76/100, Loss: 1.0887\n",
            "Epoch 77/100, Loss: 1.0885\n",
            "Epoch 78/100, Loss: 1.0885\n",
            "Epoch 79/100, Loss: 1.0874\n",
            "Epoch 80/100, Loss: 1.0879\n",
            "Epoch 81/100, Loss: 1.0875\n",
            "Epoch 82/100, Loss: 1.0873\n",
            "Epoch 83/100, Loss: 1.0875\n",
            "Epoch 84/100, Loss: 1.0874\n",
            "Epoch 85/100, Loss: 1.0874\n",
            "Epoch 86/100, Loss: 1.0872\n",
            "Epoch 87/100, Loss: 1.0873\n",
            "Epoch 88/100, Loss: 1.0871\n",
            "Epoch 89/100, Loss: 1.0869\n",
            "Epoch 90/100, Loss: 1.0871\n",
            "Epoch 91/100, Loss: 1.0869\n",
            "Epoch 92/100, Loss: 1.0869\n",
            "Epoch 93/100, Loss: 1.0868\n",
            "Epoch 94/100, Loss: 1.0869\n",
            "Epoch 95/100, Loss: 1.0870\n",
            "Epoch 96/100, Loss: 1.0867\n",
            "Epoch 97/100, Loss: 1.0869\n",
            "Epoch 98/100, Loss: 1.0866\n",
            "Epoch 99/100, Loss: 1.0867\n",
            "Epoch 100/100, Loss: 1.0866\n",
            "Train Accuracy: 0.3394\n",
            "Test Accuracy: 0.3095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# time series_lstm"
      ],
      "metadata": {
        "id": "2jD0MN6WI_Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Load METR-LA dataset\n",
        "def load_metr_la_dataset(time_series_path):\n",
        "    time_series_data = pd.read_hdf(time_series_path, \"df\")  # shape: (timesteps, nodes)\n",
        "    return time_series_data\n",
        "\n",
        "# Generate time series data\n",
        "def create_time_series_data(time_series_data, seq_length):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(len(time_series_data) - seq_length):\n",
        "        data.append(time_series_data.iloc[i:i+seq_length].values)\n",
        "        labels.append(time_series_data.iloc[i+seq_length].values)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Custom Dataset class\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        # Forward pass through LSTM\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Only take the output from the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, criterion, epochs=10, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data)\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            out = model(data)\n",
        "            predictions.append(out.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "    mse = mean_squared_error(true_labels, predictions)\n",
        "    mae = mean_absolute_error(true_labels, predictions)\n",
        "    r2 = r2_score(true_labels, predictions)\n",
        "\n",
        "    print(f\"Test MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "# Paths to METR-LA dataset\n",
        "time_series_path = \"/content/drive_mount/MyDrive/archive (29)/METR-LA.h5\"\n",
        "\n",
        "# Load dataset\n",
        "time_series_data = load_metr_la_dataset(time_series_path)\n",
        "\n",
        "# Create time series data\n",
        "seq_length = 12  # Sequence length (e.g., predicting the next value based on the last 12)\n",
        "\n",
        "# Create data and labels\n",
        "data, labels = create_time_series_data(time_series_data, seq_length)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(data) * train_ratio)\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "train_labels, test_labels = labels[:train_size], labels[train_size:]\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TimeSeriesDataset(train_data, train_labels)\n",
        "test_dataset = TimeSeriesDataset(test_data, test_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define model components\n",
        "input_dim = time_series_data.shape[1]  # Number of nodes\n",
        "hidden_dim = 64\n",
        "output_dim = time_series_data.shape[1]  # Number of nodes\n",
        "num_layers = 2\n",
        "\n",
        "model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers)\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Determine device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train and evaluate the model\n",
        "train_model(model, train_loader, optimizer, criterion, epochs=100, device=device)\n",
        "evaluate_model(model, test_loader, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHDJwsjsJD5w",
        "outputId": "415b8ac9-0f1e-4b17-987a-e6f868e4e0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 343.0768\n",
            "Epoch 2/100, Loss: 160.7804\n",
            "Epoch 3/100, Loss: 161.7779\n",
            "Epoch 4/100, Loss: 169.6268\n",
            "Epoch 5/100, Loss: 255.8895\n",
            "Epoch 6/100, Loss: 205.2028\n",
            "Epoch 7/100, Loss: 206.7097\n",
            "Epoch 8/100, Loss: 186.6997\n",
            "Epoch 9/100, Loss: 196.8815\n",
            "Epoch 10/100, Loss: 132.5844\n",
            "Epoch 11/100, Loss: 195.4702\n",
            "Epoch 12/100, Loss: 201.1268\n",
            "Epoch 13/100, Loss: 142.6893\n",
            "Epoch 14/100, Loss: 244.1316\n",
            "Epoch 15/100, Loss: 323.9764\n",
            "Epoch 16/100, Loss: 170.1968\n",
            "Epoch 17/100, Loss: 187.0440\n",
            "Epoch 18/100, Loss: 175.3933\n",
            "Epoch 19/100, Loss: 232.7141\n",
            "Epoch 20/100, Loss: 203.3325\n",
            "Epoch 21/100, Loss: 220.6606\n",
            "Epoch 22/100, Loss: 182.3668\n",
            "Epoch 23/100, Loss: 198.2322\n",
            "Epoch 24/100, Loss: 185.6296\n",
            "Epoch 25/100, Loss: 207.4905\n",
            "Epoch 26/100, Loss: 170.5513\n",
            "Epoch 27/100, Loss: 141.5580\n",
            "Epoch 28/100, Loss: 197.2070\n",
            "Epoch 29/100, Loss: 279.8860\n",
            "Epoch 30/100, Loss: 144.4003\n",
            "Epoch 31/100, Loss: 104.9193\n",
            "Epoch 32/100, Loss: 378.7969\n",
            "Epoch 33/100, Loss: 165.6007\n",
            "Epoch 34/100, Loss: 295.5452\n",
            "Epoch 35/100, Loss: 169.7561\n",
            "Epoch 36/100, Loss: 205.6374\n",
            "Epoch 37/100, Loss: 174.0716\n",
            "Epoch 38/100, Loss: 198.9285\n",
            "Epoch 39/100, Loss: 177.9281\n",
            "Epoch 40/100, Loss: 276.4999\n",
            "Epoch 41/100, Loss: 182.7297\n",
            "Epoch 42/100, Loss: 208.4390\n",
            "Epoch 43/100, Loss: 167.0742\n",
            "Epoch 44/100, Loss: 170.5395\n",
            "Epoch 45/100, Loss: 437.0930\n",
            "Epoch 46/100, Loss: 120.7907\n",
            "Epoch 47/100, Loss: 271.6025\n",
            "Epoch 48/100, Loss: 212.3918\n",
            "Epoch 49/100, Loss: 166.5246\n",
            "Epoch 50/100, Loss: 174.2982\n",
            "Epoch 51/100, Loss: 125.6983\n",
            "Epoch 52/100, Loss: 142.5331\n",
            "Epoch 53/100, Loss: 199.7942\n",
            "Epoch 54/100, Loss: 212.1209\n",
            "Epoch 55/100, Loss: 149.2841\n",
            "Epoch 56/100, Loss: 155.6158\n",
            "Epoch 57/100, Loss: 132.6526\n",
            "Epoch 58/100, Loss: 143.9190\n",
            "Epoch 59/100, Loss: 191.9491\n",
            "Epoch 60/100, Loss: 256.6160\n",
            "Epoch 61/100, Loss: 365.7589\n",
            "Epoch 62/100, Loss: 139.9306\n",
            "Epoch 63/100, Loss: 220.2957\n",
            "Epoch 64/100, Loss: 186.1394\n",
            "Epoch 65/100, Loss: 190.8305\n",
            "Epoch 66/100, Loss: 97.5588\n",
            "Epoch 67/100, Loss: 169.1271\n",
            "Epoch 68/100, Loss: 161.7213\n",
            "Epoch 69/100, Loss: 318.7088\n",
            "Epoch 70/100, Loss: 141.5138\n",
            "Epoch 71/100, Loss: 202.8524\n",
            "Epoch 72/100, Loss: 142.9848\n",
            "Epoch 73/100, Loss: 134.9092\n",
            "Epoch 74/100, Loss: 139.1655\n",
            "Epoch 75/100, Loss: 160.8566\n",
            "Epoch 76/100, Loss: 172.1102\n",
            "Epoch 77/100, Loss: 185.8595\n",
            "Epoch 78/100, Loss: 191.5676\n",
            "Epoch 79/100, Loss: 228.7540\n",
            "Epoch 80/100, Loss: 159.6718\n",
            "Epoch 81/100, Loss: 170.2981\n",
            "Epoch 82/100, Loss: 276.3751\n",
            "Epoch 83/100, Loss: 116.2772\n",
            "Epoch 84/100, Loss: 252.8339\n",
            "Epoch 85/100, Loss: 120.9924\n",
            "Epoch 86/100, Loss: 234.9236\n",
            "Epoch 87/100, Loss: 154.7549\n",
            "Epoch 88/100, Loss: 146.6581\n",
            "Epoch 89/100, Loss: 173.7351\n",
            "Epoch 90/100, Loss: 139.0322\n",
            "Epoch 91/100, Loss: 128.2941\n",
            "Epoch 92/100, Loss: 206.8118\n",
            "Epoch 93/100, Loss: 156.6229\n",
            "Epoch 94/100, Loss: 227.5004\n",
            "Epoch 95/100, Loss: 242.2184\n",
            "Epoch 96/100, Loss: 210.2182\n",
            "Epoch 97/100, Loss: 162.6215\n",
            "Epoch 98/100, Loss: 155.4905\n",
            "Epoch 99/100, Loss: 211.0909\n",
            "Epoch 100/100, Loss: 128.6442\n",
            "Test MSE: 220.5373, MAE: 8.9017, R²: 0.5703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8_jN9JLJD-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# xgboost"
      ],
      "metadata": {
        "id": "Tf-MPhBEs3ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import torch\n",
        "\n",
        "# Load METR-LA dataset\n",
        "def load_metr_la_dataset(time_series_path):\n",
        "    time_series_data = pd.read_hdf(time_series_path, \"df\")  # shape: (timesteps, nodes)\n",
        "    print(f\"Data loaded with shape: {time_series_data.shape}\")\n",
        "    return time_series_data\n",
        "\n",
        "# Generate time series data\n",
        "def create_time_series_data(time_series_data, seq_length):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(len(time_series_data) - seq_length):\n",
        "        data.append(time_series_data.iloc[i:i+seq_length].values.flatten())  # Flatten the sequence\n",
        "        labels.append(time_series_data.iloc[i+seq_length].values)  # Predict the next time step\n",
        "    print(f\"Generated data shape: {np.array(data).shape}\")\n",
        "    print(f\"Generated labels shape: {np.array(labels).shape}\")\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Paths to METR-LA dataset\n",
        "time_series_path = \"/content/drive_mount/MyDrive/archive (29)/METR-LA.h5\"\n",
        "\n",
        "# Load dataset\n",
        "time_series_data = load_metr_la_dataset(time_series_path)\n",
        "\n",
        "# Create time series data\n",
        "seq_length = 12  # Sequence length (e.g., predicting the next value based on the last 12)\n",
        "\n",
        "# Create data and labels\n",
        "data, labels = create_time_series_data(time_series_data, seq_length)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(data) * train_ratio)\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "train_labels, test_labels = labels[:train_size], labels[train_size:]\n",
        "\n",
        "# Define the model and select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# If GPU is available, use GPU for training in XGBoost\n",
        "use_gpu = (device == torch.device(\"cuda\"))\n",
        "\n",
        "# Initialize the model with GPU support if available\n",
        "model = XGBRegressor(objective='reg:squarederror', eval_metric='rmse',\n",
        "                     max_depth=6, learning_rate=0.1, n_estimators=100,\n",
        "                     tree_method='gpu_hist' if use_gpu else 'auto')\n",
        "\n",
        "# Ensure data dimensions are correct\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Training labels shape: {train_labels.shape}\")\n",
        "print(f\"Testing data shape: {test_data.shape}\")\n",
        "print(f\"Testing labels shape: {test_labels.shape}\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(test_labels, predictions)\n",
        "mae = mean_absolute_error(test_labels, predictions)\n",
        "r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "# Define regression accuracy metric (using a tolerance of 0.1 as an example)\n",
        "def regression_accuracy(y_true, y_pred, tolerance=0.1):\n",
        "    correct = np.abs(y_true - y_pred) <= tolerance\n",
        "    accuracy = np.mean(correct) * 100  # Convert to percentage\n",
        "    return accuracy\n",
        "\n",
        "# Calculate regression accuracy\n",
        "accuracy = regression_accuracy(test_labels, predictions, tolerance=0.1)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test MSE: {mse:.4f}\")\n",
        "print(f\"Test MAE: {mae:.4f}\")\n",
        "print(f\"Test R²: {r2:.4f}\")\n",
        "print(f\"Test Regression Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pszPhZc4bsz",
        "outputId": "b7715eb7-15ce-49a8-c1bd-cee8781de84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded with shape: (34272, 207)\n",
            "Generated data shape: (34260, 2484)\n",
            "Generated labels shape: (34260, 207)\n",
            "Using device: cuda\n",
            "Training data shape: (27408, 2484)\n",
            "Training labels shape: (27408, 207)\n",
            "Testing data shape: (6852, 2484)\n",
            "Testing labels shape: (6852, 207)\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:13:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:30:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:30:28] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 46.9128\n",
            "Test MAE: 3.1562\n",
            "Test R²: 0.9003\n",
            "Test Regression Accuracy: 3.41%\n"
          ]
        }
      ]
    }
  ]
}