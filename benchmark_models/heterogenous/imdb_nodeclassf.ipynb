{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khars\\Desktop\\HeteroGNN\\heterogpu\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from torch_geometric.datasets import IMDB\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_val_test_split function\n",
    "def train_val_test_split(y, val_frac=0.2, test_frac=0.2):\n",
    "    y = y.numpy().astype(int)\n",
    "    train_indices, temp_indices = train_test_split(\n",
    "        np.arange(len(y)),\n",
    "        test_size=val_frac + test_frac,\n",
    "        stratify=y\n",
    "    )\n",
    "    val_indices, test_indices = train_test_split(\n",
    "        temp_indices,\n",
    "        test_size=test_frac / (val_frac + test_frac),\n",
    "        stratify=y[temp_indices]\n",
    "    )\n",
    "    train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "    train_mask[train_indices] = True\n",
    "    val_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "    test_mask[test_indices] = True\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "transform = T.ToUndirected()\n",
    "data = IMDB(root=\"/content/drive/MyDrive/IMDB\", transform=transform)[0]\n",
    "\n",
    "# Prepare the response variable\n",
    "data[\"movie\"].y[data[\"movie\"].y == 2] = 1\n",
    "data[\"movie\"].y = data[\"movie\"].y.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_mask, val_mask, test_mask = train_val_test_split(\n",
    "    y=data[\"movie\"].y,\n",
    "    val_frac=0.2,\n",
    "    test_frac=0.2\n",
    ")\n",
    "\n",
    "# Assign masks to data\n",
    "data[\"movie\"].train_mask = train_mask\n",
    "data[\"movie\"].val_mask = val_mask\n",
    "data[\"movie\"].test_mask = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model classes\n",
    "class OutputLayer(torch.nn.Module):\n",
    "    def __init__(self, data, node_type=\"movie\"):\n",
    "        super().__init__()\n",
    "        self.node_type = node_type\n",
    "        mp_dict = {}\n",
    "\n",
    "        for meta_step in data.metadata()[1]:\n",
    "            if meta_step[2] == self.node_type:\n",
    "                src_channels = data[meta_step[0]].x.shape[1]\n",
    "                dst_channels = data[meta_step[2]].x.shape[1]\n",
    "                mp = SAGEConv((src_channels, dst_channels), 1, aggr='mean')\n",
    "                mp_dict[meta_step] = mp\n",
    "\n",
    "        # Add self-loop convolutions for node_type\n",
    "        mp = SAGEConv((data[node_type].x.shape[1], data[node_type].x.shape[1]), 1, aggr='mean')\n",
    "        mp_dict[(node_type, 'self', node_type)] = mp\n",
    "\n",
    "        self.conv = HeteroConv(mp_dict, aggr=\"sum\")\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        return self.conv(x_dict, edge_index_dict)\n",
    "\n",
    "class InnerLayer(torch.nn.Module):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        mp_dict = {}\n",
    "\n",
    "        for meta_step in data.metadata()[1]:\n",
    "            src_channels = data[meta_step[0]].x.shape[1]\n",
    "            dst_channels = data[meta_step[2]].x.shape[1]\n",
    "            mp = SAGEConv((src_channels, dst_channels), dst_channels, aggr='mean')\n",
    "            mp_dict[meta_step] = mp\n",
    "\n",
    "        # Add self-loop convolutions\n",
    "        for node_type in data.node_types:\n",
    "            mp = SAGEConv((data[node_type].x.shape[1], data[node_type].x.shape[1]), dst_channels, aggr='mean')\n",
    "            mp_dict[(node_type, 'self', node_type)] = mp\n",
    "\n",
    "        self.conv = HeteroConv(mp_dict, aggr=\"sum\")\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        return self.conv(x_dict, edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HMPNN_sum_3Layer(torch.nn.Module):\n",
    "    def __init__(self, data, node_type=\"movie\"):\n",
    "        super().__init__()\n",
    "        self.node_type = node_type\n",
    "        self.conv1 = InnerLayer(data)\n",
    "        self.conv2 = InnerLayer(data)\n",
    "        self.conv3 = OutputLayer(data, node_type=self.node_type)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict_updates = self.conv1(x_dict, edge_index_dict)\n",
    "        for node_type in x_dict_updates.keys():\n",
    "            x_dict[node_type] = torch.sigmoid(x_dict_updates[node_type])\n",
    "        x_dict_updates = self.conv2(x_dict, edge_index_dict)\n",
    "        for node_type in x_dict_updates.keys():\n",
    "            x_dict[node_type] = torch.sigmoid(x_dict_updates[node_type])\n",
    "        x_dict = self.conv3(x_dict, edge_index_dict)\n",
    "        return x_dict[self.node_type].squeeze(-1)  # Squeeze the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khars\\Desktop\\HeteroGNN\\heterogpu\\Lib\\site-packages\\torch_geometric\\nn\\conv\\hetero_conv.py:76: UserWarning: There exist node types ({'actor', 'director'}) whose representations do not get updated during message passing as they do not occur as destination type in any edge type. This may lead to unexpected behavior.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "node_type_to_classify = \"movie\"\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "min_epochs = 50\n",
    "max_epochs = 50\n",
    "print_learning_progress_freq = 50\n",
    "\n",
    "# Move model to GPU\n",
    "model = HMPNN_sum_3Layer(data, node_type_to_classify).to(device)\n",
    "\n",
    "# Move data to GPU\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_model function\n",
    "def train_model(model, data, node_type, learning_rate, weight_decay, min_epochs, max_epochs, print_freq):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    train_hist = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        loss = criterion(out[data[node_type].train_mask], data[node_type].y[data[node_type].train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_hist.append(loss.item())\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_out = model(data.x_dict, data.edge_index_dict)\n",
    "                val_loss = criterion(val_out[data[node_type].val_mask], data[node_type].y[data[node_type].val_mask])\n",
    "                print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 10:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    return model, train_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot training history\n",
    "def plot_training_hist(train_hist):\n",
    "    plt.plot(train_hist)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "def compute_accuracy(pred, y, mask):\n",
    "    pred_labels = (pred[mask].squeeze(-1) > 0.5).float()\n",
    "    correct = torch.sum(pred_labels == y[mask])\n",
    "    acc = correct / mask.sum()\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "def plot_roc_curves(data, pred, node_type):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    pred_prob = F.sigmoid(pred[data[node_type].test_mask]).cpu().numpy()\n",
    "    y_true = data[node_type].y[data[node_type].test_mask].cpu().numpy()\n",
    "    fpr, tpr, _ = roc_curve(y_true, pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, train_hist = train_model(\n",
    "    model,\n",
    "    data,\n",
    "    node_type_to_classify,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    min_epochs,\n",
    "    max_epochs,\n",
    "    print_learning_progress_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_hist(train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict)\n",
    "    train_acc = compute_accuracy(pred, data[\"movie\"].y, data[\"movie\"].train_mask)\n",
    "    val_acc = compute_accuracy(pred, data[\"movie\"].y, data[\"movie\"].val_mask)\n",
    "    test_acc = compute_accuracy(pred, data[\"movie\"].y, data[\"movie\"].test_mask)\n",
    "    print(f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plot_roc_curves(data, pred, node_type_to_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  movie={\n",
       "    x=[4278, 3066],\n",
       "    y=[4278],\n",
       "    train_mask=[4278],\n",
       "    val_mask=[4278],\n",
       "    test_mask=[4278],\n",
       "  },\n",
       "  director={ x=[2081, 3066] },\n",
       "  actor={ x=[5257, 3066] },\n",
       "  (movie, to, director)={ edge_index=[2, 4278] },\n",
       "  (movie, to, actor)={ edge_index=[2, 12828] },\n",
       "  (director, to, movie)={ edge_index=[2, 4278] },\n",
       "  (actor, to, movie)={ edge_index=[2, 12828] },\n",
       "  (director, rev_to, movie)={ edge_index=[2, 4278] },\n",
       "  (actor, rev_to, movie)={ edge_index=[2, 12828] },\n",
       "  (movie, rev_to, director)={ edge_index=[2, 4278] },\n",
       "  (movie, rev_to, actor)={ edge_index=[2, 12828] }\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import to_hetero\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch_geometric.nn import HeteroConv, NNConv, GATConv\n",
    "from torch_geometric.nn.conv import GeneralConv\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_layers, out_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Create a list to hold the layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(in_channels, hidden_layers[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], out_channels))\n",
    "        \n",
    "        # Combine all layers into a sequential model\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "        self.conv2 = GATConv((-1, -1), out_channels, add_self_loops=False)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder3(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GeneralConv((-1, -1), hidden_channels, in_edge_channels=-1)\n",
    "        self.conv2 = GeneralConv((-1, -1), out_channels, in_edge_channels=-1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, label):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        hidden_layers = [128,64, 32]\n",
    "        self.linear = MLP(in_channels, hidden_layers, out_channels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z_dict):\n",
    "        z = z_dict[self.label]\n",
    "        output = self.linear(z)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='mean')\n",
    "        self.decoder = Decoder(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        return self.decoder(z_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, label):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder2(hidden_channels, hidden_channels)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='mean')\n",
    "        self.decoder = Decoder(hidden_channels, out_channels, label)\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict, edge_attr)\n",
    "        return self.decoder(z_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, optimizer, loss_fn, data, label='movie', device='cpu', patience=50):\n",
    "    best_test_accuracy = 0.0\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data.x_dict, data.edge_index_dict, None).squeeze(1)\n",
    "        \n",
    "        # Get training masks and compute loss\n",
    "        train_mask = data[label]['train_mask']\n",
    "        train_loss = loss_fn(out[train_mask], data[label].y[train_mask])\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute training metrics\n",
    "        with torch.no_grad():\n",
    "            train_pred = out[train_mask]\n",
    "            train_true = data[label].y[train_mask].cpu()\n",
    "            train_pred_binary = (train_pred > 0.5).float()\n",
    "            train_accuracy = accuracy_score(train_true.cpu(), train_pred_binary.cpu())\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass for testing\n",
    "            test_mask = data[label]['test_mask']\n",
    "            test_pred = out[test_mask]\n",
    "            test_pred_binary = (test_pred > 0.5).float()\n",
    "            test_true = data[label].y[test_mask].cpu()\n",
    "            \n",
    "            # Compute test metrics\n",
    "            test_loss = loss_fn(out[test_mask], data[label].y[test_mask])\n",
    "            test_accuracy = accuracy_score(test_true.cpu(), test_pred_binary.cpu())\n",
    "\n",
    "        # Print epoch metrics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss.item():.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'movie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model2(hidden_channels=64, out_channels=1, label='movie')\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 0.6655, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6654, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 2/100\n",
      "Train Loss: 0.6631, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6631, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 3/100\n",
      "Train Loss: 0.6609, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6609, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 4/100\n",
      "Train Loss: 0.6587, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6588, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 5/100\n",
      "Train Loss: 0.6565, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6565, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 6/100\n",
      "Train Loss: 0.6541, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6541, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 7/100\n",
      "Train Loss: 0.6515, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6516, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 8/100\n",
      "Train Loss: 0.6488, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6490, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 9/100\n",
      "Train Loss: 0.6461, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6463, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 10/100\n",
      "Train Loss: 0.6433, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6436, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 11/100\n",
      "Train Loss: 0.6404, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6408, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 12/100\n",
      "Train Loss: 0.6370, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6376, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 13/100\n",
      "Train Loss: 0.6334, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6341, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 14/100\n",
      "Train Loss: 0.6293, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6303, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 15/100\n",
      "Train Loss: 0.6244, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6258, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 16/100\n",
      "Train Loss: 0.6184, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6204, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 17/100\n",
      "Train Loss: 0.6113, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6140, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 18/100\n",
      "Train Loss: 0.6026, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6063, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 19/100\n",
      "Train Loss: 0.5924, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5975, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 20/100\n",
      "Train Loss: 0.5811, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5878, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 21/100\n",
      "Train Loss: 0.5690, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5777, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 22/100\n",
      "Train Loss: 0.5558, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5669, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 23/100\n",
      "Train Loss: 0.5416, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5558, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 24/100\n",
      "Train Loss: 0.5271, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5453, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 25/100\n",
      "Train Loss: 0.5132, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5363, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 26/100\n",
      "Train Loss: 0.5007, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5300, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 27/100\n",
      "Train Loss: 0.4904, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5269, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 28/100\n",
      "Train Loss: 0.4820, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5269, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 29/100\n",
      "Train Loss: 0.4742, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5282, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 30/100\n",
      "Train Loss: 0.4653, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5282, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 31/100\n",
      "Train Loss: 0.4539, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5250, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 32/100\n",
      "Train Loss: 0.4398, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5177, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 33/100\n",
      "Train Loss: 0.4238, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5072, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 34/100\n",
      "Train Loss: 0.4075, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4949, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 35/100\n",
      "Train Loss: 0.3921, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4827, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 36/100\n",
      "Train Loss: 0.3782, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4717, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 37/100\n",
      "Train Loss: 0.3662, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4627, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 38/100\n",
      "Train Loss: 0.3561, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4562, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 39/100\n",
      "Train Loss: 0.3472, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4520, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 40/100\n",
      "Train Loss: 0.3388, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4503, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 41/100\n",
      "Train Loss: 0.3305, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4508, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 42/100\n",
      "Train Loss: 0.3219, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4538, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 43/100\n",
      "Train Loss: 0.3132, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4598, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 44/100\n",
      "Train Loss: 0.3046, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4692, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 45/100\n",
      "Train Loss: 0.2962, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4825, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 46/100\n",
      "Train Loss: 0.2884, Train Accuracy: 0.7346\n",
      "Test Loss: 0.4994, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 47/100\n",
      "Train Loss: 0.2810, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5197, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 48/100\n",
      "Train Loss: 0.2740, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5426, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 49/100\n",
      "Train Loss: 0.2673, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5671, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 50/100\n",
      "Train Loss: 0.2607, Train Accuracy: 0.7346\n",
      "Test Loss: 0.5923, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 51/100\n",
      "Train Loss: 0.2542, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6175, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 52/100\n",
      "Train Loss: 0.2479, Train Accuracy: 0.7346\n",
      "Test Loss: 0.6425, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 53/100\n",
      "Train Loss: 0.2419, Train Accuracy: 0.7350\n",
      "Test Loss: 0.6671, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 54/100\n",
      "Train Loss: 0.2360, Train Accuracy: 0.7362\n",
      "Test Loss: 0.6921, Test Accuracy: 0.7348\n",
      "----------------------------------------\n",
      "Epoch 55/100\n",
      "Train Loss: 0.2303, Train Accuracy: 0.7405\n",
      "Test Loss: 0.7174, Test Accuracy: 0.7360\n",
      "----------------------------------------\n",
      "Epoch 56/100\n",
      "Train Loss: 0.2247, Train Accuracy: 0.7486\n",
      "Test Loss: 0.7442, Test Accuracy: 0.7371\n",
      "----------------------------------------\n",
      "Epoch 57/100\n",
      "Train Loss: 0.2192, Train Accuracy: 0.7728\n",
      "Test Loss: 0.7721, Test Accuracy: 0.7547\n",
      "----------------------------------------\n",
      "Epoch 58/100\n",
      "Train Loss: 0.2141, Train Accuracy: 0.8176\n",
      "Test Loss: 0.8977, Test Accuracy: 0.7652\n",
      "----------------------------------------\n",
      "Epoch 59/100\n",
      "Train Loss: 0.2092, Train Accuracy: 0.8523\n",
      "Test Loss: 0.9255, Test Accuracy: 0.7862\n",
      "----------------------------------------\n",
      "Epoch 60/100\n",
      "Train Loss: 0.2045, Train Accuracy: 0.8819\n",
      "Test Loss: 1.0505, Test Accuracy: 0.7979\n",
      "----------------------------------------\n",
      "Epoch 61/100\n",
      "Train Loss: 0.1998, Train Accuracy: 0.9104\n",
      "Test Loss: 1.0748, Test Accuracy: 0.8061\n",
      "----------------------------------------\n",
      "Epoch 62/100\n",
      "Train Loss: 0.1949, Train Accuracy: 0.9295\n",
      "Test Loss: 1.1949, Test Accuracy: 0.8084\n",
      "----------------------------------------\n",
      "Epoch 63/100\n",
      "Train Loss: 0.1899, Train Accuracy: 0.9497\n",
      "Test Loss: 1.3138, Test Accuracy: 0.8107\n",
      "----------------------------------------\n",
      "Epoch 64/100\n",
      "Train Loss: 0.1846, Train Accuracy: 0.9696\n",
      "Test Loss: 1.3335, Test Accuracy: 0.8166\n",
      "----------------------------------------\n",
      "Epoch 65/100\n",
      "Train Loss: 0.1789, Train Accuracy: 0.9797\n",
      "Test Loss: 1.3500, Test Accuracy: 0.8189\n",
      "----------------------------------------\n",
      "Epoch 66/100\n",
      "Train Loss: 0.1730, Train Accuracy: 0.9848\n",
      "Test Loss: 1.4613, Test Accuracy: 0.8213\n",
      "----------------------------------------\n",
      "Epoch 67/100\n",
      "Train Loss: 0.1665, Train Accuracy: 0.9856\n",
      "Test Loss: 1.4719, Test Accuracy: 0.8283\n",
      "----------------------------------------\n",
      "Epoch 68/100\n",
      "Train Loss: 0.1594, Train Accuracy: 0.9860\n",
      "Test Loss: 1.4781, Test Accuracy: 0.8294\n",
      "----------------------------------------\n",
      "Epoch 69/100\n",
      "Train Loss: 0.1515, Train Accuracy: 0.9860\n",
      "Test Loss: 1.4797, Test Accuracy: 0.8236\n",
      "----------------------------------------\n",
      "Epoch 70/100\n",
      "Train Loss: 0.1428, Train Accuracy: 0.9860\n",
      "Test Loss: 1.4759, Test Accuracy: 0.8248\n",
      "----------------------------------------\n",
      "Epoch 71/100\n",
      "Train Loss: 0.1334, Train Accuracy: 0.9860\n",
      "Test Loss: 1.4653, Test Accuracy: 0.8213\n",
      "----------------------------------------\n",
      "Epoch 72/100\n",
      "Train Loss: 0.1233, Train Accuracy: 0.9860\n",
      "Test Loss: 1.4507, Test Accuracy: 0.8189\n",
      "----------------------------------------\n",
      "Epoch 73/100\n",
      "Train Loss: 0.1128, Train Accuracy: 0.9864\n",
      "Test Loss: 1.4317, Test Accuracy: 0.8189\n",
      "----------------------------------------\n",
      "Epoch 74/100\n",
      "Train Loss: 0.1024, Train Accuracy: 0.9864\n",
      "Test Loss: 1.4140, Test Accuracy: 0.8178\n",
      "----------------------------------------\n",
      "Epoch 75/100\n",
      "Train Loss: 0.0923, Train Accuracy: 0.9864\n",
      "Test Loss: 1.4988, Test Accuracy: 0.8154\n",
      "----------------------------------------\n",
      "Epoch 76/100\n",
      "Train Loss: 0.0829, Train Accuracy: 0.9864\n",
      "Test Loss: 1.4908, Test Accuracy: 0.8096\n",
      "----------------------------------------\n",
      "Epoch 77/100\n",
      "Train Loss: 0.0744, Train Accuracy: 0.9864\n",
      "Test Loss: 1.4907, Test Accuracy: 0.8072\n",
      "----------------------------------------\n",
      "Epoch 78/100\n",
      "Train Loss: 0.0669, Train Accuracy: 0.9864\n",
      "Test Loss: 1.5986, Test Accuracy: 0.8026\n",
      "----------------------------------------\n",
      "Epoch 79/100\n",
      "Train Loss: 0.0605, Train Accuracy: 0.9864\n",
      "Test Loss: 1.6142, Test Accuracy: 0.8026\n",
      "----------------------------------------\n",
      "Epoch 80/100\n",
      "Train Loss: 0.0554, Train Accuracy: 0.9864\n",
      "Test Loss: 1.7340, Test Accuracy: 0.8072\n",
      "----------------------------------------\n",
      "Epoch 81/100\n",
      "Train Loss: 0.0514, Train Accuracy: 0.9864\n",
      "Test Loss: 1.7597, Test Accuracy: 0.8072\n",
      "----------------------------------------\n",
      "Epoch 82/100\n",
      "Train Loss: 0.0484, Train Accuracy: 0.9864\n",
      "Test Loss: 1.8863, Test Accuracy: 0.8096\n",
      "----------------------------------------\n",
      "Epoch 83/100\n",
      "Train Loss: 0.0459, Train Accuracy: 0.9864\n",
      "Test Loss: 2.0137, Test Accuracy: 0.8072\n",
      "----------------------------------------\n",
      "Epoch 84/100\n",
      "Train Loss: 0.0437, Train Accuracy: 0.9864\n",
      "Test Loss: 2.0425, Test Accuracy: 0.8061\n",
      "----------------------------------------\n",
      "Epoch 85/100\n",
      "Train Loss: 0.0414, Train Accuracy: 0.9867\n",
      "Test Loss: 2.4622, Test Accuracy: 0.8084\n",
      "----------------------------------------\n",
      "Epoch 86/100\n",
      "Train Loss: 0.0395, Train Accuracy: 0.9867\n",
      "Test Loss: 2.7835, Test Accuracy: 0.8131\n",
      "----------------------------------------\n",
      "Epoch 87/100\n",
      "Train Loss: 0.0369, Train Accuracy: 0.9875\n",
      "Test Loss: 2.9071, Test Accuracy: 0.8119\n",
      "----------------------------------------\n",
      "Epoch 88/100\n",
      "Train Loss: 0.0339, Train Accuracy: 0.9883\n",
      "Test Loss: 2.9336, Test Accuracy: 0.8143\n",
      "----------------------------------------\n",
      "Epoch 89/100\n",
      "Train Loss: 0.0303, Train Accuracy: 0.9891\n",
      "Test Loss: 3.2577, Test Accuracy: 0.8119\n",
      "----------------------------------------\n",
      "Epoch 90/100\n",
      "Train Loss: 0.0258, Train Accuracy: 0.9918\n",
      "Test Loss: 3.2869, Test Accuracy: 0.8131\n",
      "----------------------------------------\n",
      "Early stopping triggered at epoch 90\n"
     ]
    }
   ],
   "source": [
    "trained_model, best_accuracy = train(\n",
    "    num_epochs=100, \n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    loss_fn=loss_fn, \n",
    "    data=data,  # Your HeteroData object\n",
    "    label='movie'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "heterogpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
