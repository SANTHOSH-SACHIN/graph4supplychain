{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79150e2e-1a2f-43e3-8319-00f0a1b29f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bef7cdd-d788-4659-ba63-6955490326c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_data(sequence_length, num_of_depend, label_start_idx,num_for_predict, units, points_per_hour):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence_length: int, length of all history data\n",
    "    num_of_depend: int,\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int, the number of points will be predicted for each sample\n",
    "    units: int, week: 7 * 24, day: 24, recent(hour): 1\n",
    "    points_per_hour: int, number of points per hour, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    list[(start_idx, end_idx)]\n",
    "    '''\n",
    "\n",
    "    if points_per_hour < 0:\n",
    "        raise ValueError(\"points_per_hour should be greater than 0!\")\n",
    "\n",
    "    if label_start_idx + num_for_predict > sequence_length:\n",
    "        return None\n",
    "\n",
    "    x_idx = []\n",
    "    for i in range(1, num_of_depend + 1):\n",
    "        start_idx = label_start_idx - points_per_hour * units * i\n",
    "        end_idx = start_idx + num_for_predict\n",
    "        if start_idx >= 0:\n",
    "            x_idx.append((start_idx, end_idx))\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    if len(x_idx) != num_of_depend:\n",
    "        return None\n",
    "\n",
    "    return x_idx[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b14ae599-481b-4916-a028-3ab40c7251e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_indices(data_sequence, num_of_weeks, num_of_days, num_of_hours, label_start_idx, num_for_predict, points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_sequence: np.ndarray shape is (sequence_length, num_of_vertices, num_of_features)\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    label_start_idx: int, the first index of predicting target\n",
    "    num_for_predict: int,the number of points will be predicted for each sample\n",
    "    points_per_hour: int, default 12, number of points per hour\n",
    "    Returns\n",
    "    ----------\n",
    "    week_sample: np.ndarray shape is (num_of_weeks * points_per_hour, num_of_vertices, num_of_features)\n",
    "    day_sample: np.ndarray shape is (num_of_days * points_per_hour,  num_of_vertices, num_of_features)\n",
    "    hour_sample: np.ndarray   shape is (num_of_hours * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray shape is (num_for_predict, num_of_vertices, num_of_features)\n",
    "    '''\n",
    "    week_sample, day_sample, hour_sample = None, None, None\n",
    "#------------------------------------Ignore\n",
    "    if label_start_idx + num_for_predict > data_sequence.shape[0]: \n",
    "        return week_sample, day_sample, hour_sample, None\n",
    "\n",
    "    if num_of_weeks > 0:\n",
    "        week_indices = search_data(data_sequence.shape[0], num_of_weeks, label_start_idx, num_for_predict,7 * 24, points_per_hour)\n",
    "        if not week_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        week_sample = np.concatenate([data_sequence[i: j] for i, j in week_indices], axis=0)\n",
    "\n",
    "    if num_of_days > 0:\n",
    "        day_indices = search_data(data_sequence.shape[0], num_of_days,  label_start_idx, num_for_predict, 24, points_per_hour)\n",
    "        if not day_indices:\n",
    "            return None, None, None, None\n",
    "\n",
    "        day_sample = np.concatenate([data_sequence[i: j] for i, j in day_indices], axis=0)\n",
    "#----------------------------------Continue\n",
    "    if num_of_hours > 0:\n",
    "        hour_indices = search_data(data_sequence.shape[0], num_of_hours, label_start_idx, num_for_predict, 1, points_per_hour)\n",
    "        if not hour_indices:\n",
    "            return None, None, None, None\n",
    "        hour_sample = np.concatenate([data_sequence[i: j] for i, j in hour_indices], axis=0)\n",
    "    \n",
    "    if num_of_hours > 10:\n",
    "        return 1;\n",
    "    target = data_sequence[label_start_idx: label_start_idx + num_for_predict]\n",
    "\n",
    "    return week_sample, day_sample, hour_sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb5459a8-b370-4f9a-972d-95e2d93b4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_generate_dataset(graph_signal_matrix_filename, num_of_weeks, num_of_days, num_of_hours, num_for_predict, points_per_hour=12):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_signal_matrix_filename: str, path of graph signal matrix file\n",
    "    num_of_weeks, num_of_days, num_of_hours: int\n",
    "    num_for_predict: int\n",
    "    points_per_hour: int, default 12, depends on data\n",
    "    Returns\n",
    "    ----------\n",
    "    feature: np.ndarray, shape is (num_of_samples, num_of_depend * points_per_hour, num_of_vertices, num_of_features)\n",
    "    target: np.ndarray, shape is (num_of_samples, num_of_vertices, num_for_predict)\n",
    "    '''\n",
    "    #--------------------------------- Read original data \n",
    "    data_seq = np.load(graph_signal_matrix_filename)['data']  # (sequence_length, num_of_vertices, num_of_features) (16992, 307, 3)\n",
    "    \n",
    "    #---------------------------------\n",
    "    all_samples = []\n",
    "    for idx in range(data_seq.shape[0]):\n",
    "        sample = get_sample_indices(data_seq, num_of_weeks, num_of_days, num_of_hours, idx, num_for_predict, points_per_hour)\n",
    "        if ((sample[0] is None) and (sample[1] is None) and (sample[2] is None)):\n",
    "            continue\n",
    "\n",
    "        week_sample, day_sample, hour_sample, target = sample #  week_sample, day_sample are None because we are predicting per hour\n",
    "        #print(target.shape) # hour_sample and target (12, 307, 3)\n",
    "        sample = []  # [(week_sample),(day_sample),(hour_sample),target,time_sample]\n",
    "#-------------------------------- Ignore\n",
    "        if num_of_weeks > 0:\n",
    "            week_sample = np.expand_dims(week_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(week_sample)\n",
    "\n",
    "        if num_of_days > 0:\n",
    "            day_sample = np.expand_dims(day_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(day_sample)\n",
    "#----------------------------------Continue\n",
    "        if num_of_hours > 0:\n",
    "            hour_sample = np.expand_dims(hour_sample, axis=0).transpose((0, 2, 3, 1))  # (1,N,F,T)\n",
    "            sample.append(hour_sample)\n",
    "\n",
    "        target = np.expand_dims(target, axis=0).transpose((0, 2, 3, 1))[:, :, 0, :]  # (1,N,T)\n",
    "        sample.append(target)\n",
    "        time_sample = np.expand_dims(np.array([idx]), axis=0)  # (1,1)\n",
    "        sample.append(time_sample)\n",
    "        all_samples.append(sample)#sampeï¼š[(week_sample),(day_sample),(hour_sample),target,time_sample] = [(1,N,F,Tw),(1,N,F,Td),(1,N,F,Th),(1,N,Tpre),(1,1)]\n",
    "\n",
    "    split_line1 = int(len(all_samples) * 0.6)\n",
    "    split_line2 = int(len(all_samples) * 0.8)\n",
    "\n",
    "    training_set = [np.concatenate(i, axis=0)  for i in zip(*all_samples[:split_line1])] #[(B,N,F,Tw),(B,N,F,Td),(B,N,F,Th),(B,N,Tpre),(B,1)]\n",
    "    validation_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line1: split_line2])]\n",
    "    testing_set = [np.concatenate(i, axis=0) for i in zip(*all_samples[split_line2:])]\n",
    "\n",
    "    return training_set, validation_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d623370-0e56-4030-a171-f391faf631bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28224, 883, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_signal_matrix_filename = '/Users/varun/Desktop/finalYear/code/data/peMSD7_npz/PEMSd7.npz'\n",
    "data = np.load(graph_signal_matrix_filename)\n",
    "data['data'].shape # 2nd value is num_vertices: (28224, 883, 1) ->883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dba548b-5380-4bad-9b64-e48c38991190",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(graph_signal_matrix_filename)\n",
    "data['data'].shape\n",
    "\n",
    "num_of_vertices = 883\n",
    "points_per_hour = 12\n",
    "num_for_predict = 12\n",
    "num_of_weeks = 0\n",
    "num_of_days = 0\n",
    "num_of_hours = 1\n",
    "\n",
    "training_set, validation_set, testing_set = read_and_generate_dataset(graph_signal_matrix_filename, 0, 0, num_of_hours, \n",
    "                                                                      num_for_predict, points_per_hour=points_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7618c9dd-9c61-49b3-b4d1-e553c3a05cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train, val, test):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train, val, test: np.ndarray (B,N,F,T)\n",
    "    Returns\n",
    "    ----------\n",
    "    stats: dict, two keys: mean and std\n",
    "    train_norm, val_norm, test_norm: np.ndarray,\n",
    "                                     shape is the same as original\n",
    "    '''\n",
    "\n",
    "    assert train.shape[1:] == val.shape[1:] and val.shape[1:] == test.shape[1:]  # ensure the num of nodes is the same\n",
    "    mean = train.mean(axis=(0,1,3), keepdims=True)\n",
    "    std = train.std(axis=(0,1,3), keepdims=True)\n",
    "    print('mean.shape:',mean.shape)\n",
    "    print('std.shape:',std.shape)\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "\n",
    "    train_norm = normalize(train)\n",
    "    val_norm = normalize(val)\n",
    "    test_norm = normalize(test)\n",
    "\n",
    "    return {'_mean': mean, '_std': std}, train_norm, val_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c85b022d-4cd4-4d31-91f9-1fc7acef4598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean.shape: (1, 1, 1, 1)\n",
      "std.shape: (1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = np.concatenate(training_set[:-2], axis=-1)  # (B,N,F,T')\n",
    "val_x = np.concatenate(validation_set[:-2], axis=-1)\n",
    "test_x = np.concatenate(testing_set[:-2], axis=-1)\n",
    "\n",
    "train_target = training_set[-2]  # (B,N,T)\n",
    "val_target = validation_set[-2]\n",
    "test_target = testing_set[-2]\n",
    "\n",
    "train_timestamp = training_set[-1]  # (B,1)\n",
    "val_timestamp = validation_set[-1]\n",
    "test_timestamp = testing_set[-1]\n",
    "\n",
    "(stats, train_x_norm, val_x_norm, test_x_norm) = normalization(train_x, val_x, test_x)\n",
    "\n",
    "all_data = {'train': { 'x': train_x_norm, 'target': train_target,'timestamp': train_timestamp},\n",
    "            'val': {'x': val_x_norm, 'target': val_target, 'timestamp': val_timestamp},\n",
    "            'test': {'x': test_x_norm, 'target': test_target, 'timestamp': test_timestamp},\n",
    "            'stats': {'_mean': stats['_mean'], '_std': stats['_std']} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86ebdbbd-43d3-49e0-bc1d-5ca5dace5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x: (16920, 883, 1, 12)\n",
      "train target: (16920, 883, 12)\n",
      "train timestamp: (16920, 1)\n",
      "\n",
      "val x: (5640, 883, 1, 12)\n",
      "val target: (5640, 883, 12)\n",
      "val timestamp: (5640, 1)\n",
      "\n",
      "test x: (5641, 883, 1, 12)\n",
      "test target: (5641, 883, 12)\n",
      "test timestamp: (5641, 1)\n",
      "\n",
      "train data _mean : (1, 1, 1, 1) [[[[309.53543959]]]]\n",
      "train data _std : (1, 1, 1, 1) [[[[189.50781284]]]]\n"
     ]
    }
   ],
   "source": [
    "print('train x:', all_data['train']['x'].shape)\n",
    "print('train target:', all_data['train']['target'].shape)\n",
    "print('train timestamp:', all_data['train']['timestamp'].shape)\n",
    "print()\n",
    "print('val x:', all_data['val']['x'].shape)\n",
    "print('val target:', all_data['val']['target'].shape)\n",
    "print('val timestamp:', all_data['val']['timestamp'].shape)\n",
    "print()\n",
    "print('test x:', all_data['test']['x'].shape)\n",
    "print('test target:', all_data['test']['target'].shape)\n",
    "print('test timestamp:', all_data['test']['timestamp'].shape)\n",
    "print()\n",
    "print('train data _mean :', all_data['stats']['_mean'].shape, all_data['stats']['_mean'])\n",
    "print('train data _std :', all_data['stats']['_std'].shape, all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e593909-7ac3-4dc5-a60c-7b5d6731d936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save file: /Users/varun/Desktop/finalYear/code/data/peMSD7_npz/PEMSd7_45m_r1_d0_w0_a3tgcn\n"
     ]
    }
   ],
   "source": [
    "file = os.path.basename(graph_signal_matrix_filename).split('.')[0]\n",
    "dirpath = '/Users/varun/Desktop/finalYear/code/data/peMSD7_npz'\n",
    "filename = os.path.join(dirpath, file +'_45m' +'_r'  +str(num_of_hours) + '_d' + str(num_of_days) + '_w' + str(num_of_weeks)) + '_a3tgcn'\n",
    "print('save file:', filename)\n",
    "np.savez_compressed(filename,\n",
    "                train_x=all_data['train']['x'],train_target=all_data['train']['target'],train_timestamp=all_data['train']['timestamp'],\n",
    "                val_x=all_data['val']['x'], val_target=all_data['val']['target'],val_timestamp=all_data['val']['timestamp'],\n",
    "                test_x=all_data['test']['x'], test_target=all_data['test']['target'], test_timestamp=all_data['test']['timestamp'],\n",
    "                mean=all_data['stats']['_mean'], std=all_data['stats']['_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095d5a3-0d96-4385-af2f-ed257709d11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lam)",
   "language": "python",
   "name": "lam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
