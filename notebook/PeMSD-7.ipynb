{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_layers, out_channels):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_layer = in_channels\n",
    "\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_layer, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_layer = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer, out_channels))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_gru=64):\n",
    "        super().__init__()\n",
    "        hidden_layers = [128, 64, 32]\n",
    "\n",
    "        self.gru = nn.GRU(in_channels, hidden_gru, batch_first=True)\n",
    "        self.mlp = MLP(hidden_gru, hidden_layers, out_channels)\n",
    "\n",
    "    def forward(self, z):\n",
    "        gru_out, _ = self.gru(z)\n",
    "        output = self.mlp(gru_out.squeeze())\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_channels) #linear encoder\n",
    "        self.decoder = GRUDecoder(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.unsqueeze(1)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    speed_data = pd.read_csv('dataset/PeMSD7_V_228.csv', header=None)\n",
    "\n",
    "    adjacency_matrix = pd.read_csv('dataset/PeMSD7_W_228.csv', header=None)\n",
    "except FileNotFoundError:\n",
    "    import os\n",
    "    import requests\n",
    "    import zipfile\n",
    "\n",
    "\n",
    "    url = \"https://github.com/VeritasYin/STGCN_IJCAI-18/raw/master/dataset/PeMSD7_Full.zip\"\n",
    "\n",
    "    os.makedirs('dataset', exist_ok=True)\n",
    "    response = requests.get(url)\n",
    "    zip_path = 'dataset/PeMSD7_Full.zip'\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('dataset')\n",
    "\n",
    "\n",
    "    speed_data = pd.read_csv('dataset/PeMSD7_V_228.csv', header=None)\n",
    "\n",
    "\n",
    "    adjacency_matrix = pd.read_csv('dataset/PeMSD7_W_228.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "speed_data = scaler.fit_transform(speed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(speed_data) * 0.8)\n",
    "train_data = speed_data[:train_size]\n",
    "test_data = speed_data[train_size:]\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = speed_data.shape[1]  # Number of sensors\n",
    "hidden_channels = 64\n",
    "out_channels = speed_data.shape[1]\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, hidden_channels, out_channels)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.005492925643921\n",
      "Epoch 2/50, Loss: 1.0036362409591675\n",
      "Epoch 3/50, Loss: 1.002098560333252\n",
      "Epoch 4/50, Loss: 1.000372290611267\n",
      "Epoch 5/50, Loss: 0.9982897043228149\n",
      "Epoch 6/50, Loss: 0.9958019256591797\n",
      "Epoch 7/50, Loss: 0.9928260445594788\n",
      "Epoch 8/50, Loss: 0.9893143773078918\n",
      "Epoch 9/50, Loss: 0.9852292537689209\n",
      "Epoch 10/50, Loss: 0.9804682731628418\n",
      "Epoch 11/50, Loss: 0.9749297499656677\n",
      "Epoch 12/50, Loss: 0.9685161113739014\n",
      "Epoch 13/50, Loss: 0.9612706899642944\n",
      "Epoch 14/50, Loss: 0.9532877802848816\n",
      "Epoch 15/50, Loss: 0.9448174834251404\n",
      "Epoch 16/50, Loss: 0.936309814453125\n",
      "Epoch 17/50, Loss: 0.9281448721885681\n",
      "Epoch 18/50, Loss: 0.9204778671264648\n",
      "Epoch 19/50, Loss: 0.9130458235740662\n",
      "Epoch 20/50, Loss: 0.9051369428634644\n",
      "Epoch 21/50, Loss: 0.8960779309272766\n",
      "Epoch 22/50, Loss: 0.8859115839004517\n",
      "Epoch 23/50, Loss: 0.8749741315841675\n",
      "Epoch 24/50, Loss: 0.8636137843132019\n",
      "Epoch 25/50, Loss: 0.8520757555961609\n",
      "Epoch 26/50, Loss: 0.8403916954994202\n",
      "Epoch 27/50, Loss: 0.8284321427345276\n",
      "Epoch 28/50, Loss: 0.8160187602043152\n",
      "Epoch 29/50, Loss: 0.8031002879142761\n",
      "Epoch 30/50, Loss: 0.7899608612060547\n",
      "Epoch 31/50, Loss: 0.7766315937042236\n",
      "Epoch 32/50, Loss: 0.7633457183837891\n",
      "Epoch 33/50, Loss: 0.750186026096344\n",
      "Epoch 34/50, Loss: 0.7370629906654358\n",
      "Epoch 35/50, Loss: 0.723683774471283\n",
      "Epoch 36/50, Loss: 0.7099649906158447\n",
      "Epoch 37/50, Loss: 0.6960009336471558\n",
      "Epoch 38/50, Loss: 0.6825528144836426\n",
      "Epoch 39/50, Loss: 0.6690265536308289\n",
      "Epoch 40/50, Loss: 0.6552276015281677\n",
      "Epoch 41/50, Loss: 0.641887903213501\n",
      "Epoch 42/50, Loss: 0.6293075084686279\n",
      "Epoch 43/50, Loss: 0.6174243092536926\n",
      "Epoch 44/50, Loss: 0.6059569120407104\n",
      "Epoch 45/50, Loss: 0.5945342779159546\n",
      "Epoch 46/50, Loss: 0.5835115313529968\n",
      "Epoch 47/50, Loss: 0.5727127194404602\n",
      "Epoch 48/50, Loss: 0.5618095993995667\n",
      "Epoch 49/50, Loss: 0.5513327717781067\n",
      "Epoch 50/50, Loss: 0.5411383509635925\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Prepare input and target\n",
    "    input_data = train_data[:-1]\n",
    "    target_data = train_data[1:]\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = loss_fn(output, target_data)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.564700186252594\n",
      "MAE: 0.4765739440917969\n",
      "MAPE: 231.72047424316406\n",
      "RMSE: 0.7514653593164451\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Prepare input and target\n",
    "    input_data = test_data[:-1]\n",
    "    target_data = test_data[1:]\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(target_data.numpy(), output.numpy())\n",
    "    mae = mean_absolute_error(target_data.numpy(), output.numpy())\n",
    "    mape = mean_absolute_percentage_error(target_data.numpy(), output.numpy())\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'MAE: {mae}')\n",
    "    print(f'MAPE: {mape}')\n",
    "    print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
